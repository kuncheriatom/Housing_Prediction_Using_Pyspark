{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with maximum configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"loan_default\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.num.executors\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SK_ID_CURR: double (nullable = true)\n",
      " |-- TARGET: integer (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE: string (nullable = true)\n",
      " |-- CODE_GENDER: string (nullable = true)\n",
      " |-- FLAG_OWN_CAR: string (nullable = true)\n",
      " |-- FLAG_OWN_REALTY: string (nullable = true)\n",
      " |-- CNT_CHILDREN: double (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: double (nullable = true)\n",
      " |-- AMT_CREDIT: double (nullable = true)\n",
      " |-- AMT_ANNUITY: double (nullable = true)\n",
      " |-- AMT_GOODS_PRICE: double (nullable = true)\n",
      " |-- NAME_TYPE_SUITE: string (nullable = true)\n",
      " |-- NAME_INCOME_TYPE: string (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE: string (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS: string (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE: string (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: double (nullable = true)\n",
      " |-- DAYS_BIRTH: double (nullable = true)\n",
      " |-- DAYS_EMPLOYED: double (nullable = true)\n",
      " |-- DAYS_REGISTRATION: double (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: double (nullable = true)\n",
      " |-- OWN_CAR_AGE: double (nullable = true)\n",
      " |-- FLAG_MOBIL: double (nullable = true)\n",
      " |-- FLAG_EMP_PHONE: double (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: double (nullable = true)\n",
      " |-- FLAG_CONT_MOBILE: double (nullable = true)\n",
      " |-- FLAG_PHONE: double (nullable = true)\n",
      " |-- FLAG_EMAIL: double (nullable = true)\n",
      " |-- OCCUPATION_TYPE: string (nullable = true)\n",
      " |-- CNT_FAM_MEMBERS: double (nullable = true)\n",
      " |-- REGION_RATING_CLIENT: double (nullable = true)\n",
      " |-- REGION_RATING_CLIENT_W_CITY: double (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START: string (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: double (nullable = true)\n",
      " |-- REG_REGION_NOT_LIVE_REGION: double (nullable = true)\n",
      " |-- REG_REGION_NOT_WORK_REGION: double (nullable = true)\n",
      " |-- LIVE_REGION_NOT_WORK_REGION: double (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: double (nullable = true)\n",
      " |-- REG_CITY_NOT_WORK_CITY: double (nullable = true)\n",
      " |-- LIVE_CITY_NOT_WORK_CITY: double (nullable = true)\n",
      " |-- ORGANIZATION_TYPE: string (nullable = true)\n",
      " |-- EXT_SOURCE_1: double (nullable = true)\n",
      " |-- EXT_SOURCE_2: double (nullable = true)\n",
      " |-- EXT_SOURCE_3: double (nullable = true)\n",
      " |-- APARTMENTS_AVG: double (nullable = true)\n",
      " |-- BASEMENTAREA_AVG: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_AVG: double (nullable = true)\n",
      " |-- YEARS_BUILD_AVG: double (nullable = true)\n",
      " |-- COMMONAREA_AVG: double (nullable = true)\n",
      " |-- ELEVATORS_AVG: double (nullable = true)\n",
      " |-- ENTRANCES_AVG: double (nullable = true)\n",
      " |-- FLOORSMAX_AVG: double (nullable = true)\n",
      " |-- FLOORSMIN_AVG: double (nullable = true)\n",
      " |-- LANDAREA_AVG: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_AVG: double (nullable = true)\n",
      " |-- LIVINGAREA_AVG: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_AVG: double (nullable = true)\n",
      " |-- NONLIVINGAREA_AVG: double (nullable = true)\n",
      " |-- APARTMENTS_MODE: double (nullable = true)\n",
      " |-- BASEMENTAREA_MODE: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MODE: double (nullable = true)\n",
      " |-- YEARS_BUILD_MODE: double (nullable = true)\n",
      " |-- COMMONAREA_MODE: double (nullable = true)\n",
      " |-- ELEVATORS_MODE: double (nullable = true)\n",
      " |-- ENTRANCES_MODE: double (nullable = true)\n",
      " |-- FLOORSMAX_MODE: double (nullable = true)\n",
      " |-- FLOORSMIN_MODE: double (nullable = true)\n",
      " |-- LANDAREA_MODE: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_MODE: double (nullable = true)\n",
      " |-- LIVINGAREA_MODE: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_MODE: double (nullable = true)\n",
      " |-- NONLIVINGAREA_MODE: double (nullable = true)\n",
      " |-- APARTMENTS_MEDI: double (nullable = true)\n",
      " |-- BASEMENTAREA_MEDI: double (nullable = true)\n",
      " |-- YEARS_BEGINEXPLUATATION_MEDI: double (nullable = true)\n",
      " |-- YEARS_BUILD_MEDI: double (nullable = true)\n",
      " |-- COMMONAREA_MEDI: double (nullable = true)\n",
      " |-- ELEVATORS_MEDI: double (nullable = true)\n",
      " |-- ENTRANCES_MEDI: double (nullable = true)\n",
      " |-- FLOORSMAX_MEDI: double (nullable = true)\n",
      " |-- FLOORSMIN_MEDI: double (nullable = true)\n",
      " |-- LANDAREA_MEDI: double (nullable = true)\n",
      " |-- LIVINGAPARTMENTS_MEDI: double (nullable = true)\n",
      " |-- LIVINGAREA_MEDI: double (nullable = true)\n",
      " |-- NONLIVINGAPARTMENTS_MEDI: double (nullable = true)\n",
      " |-- NONLIVINGAREA_MEDI: double (nullable = true)\n",
      " |-- FONDKAPREMONT_MODE: string (nullable = true)\n",
      " |-- HOUSETYPE_MODE: string (nullable = true)\n",
      " |-- TOTALAREA_MODE: double (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE: string (nullable = true)\n",
      " |-- EMERGENCYSTATE_MODE: string (nullable = true)\n",
      " |-- OBS_30_CNT_SOCIAL_CIRCLE: double (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: double (nullable = true)\n",
      " |-- OBS_60_CNT_SOCIAL_CIRCLE: double (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: double (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_2: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_4: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_5: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_6: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_7: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_8: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_9: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_10: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_11: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_12: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_14: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_15: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_17: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_19: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_20: double (nullable = true)\n",
      " |-- FLAG_DOCUMENT_21: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_HOUR: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_DAY: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_WEEK: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: double (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: double (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE_index: double (nullable = true)\n",
      " |-- CODE_GENDER_index: double (nullable = true)\n",
      " |-- FLAG_OWN_CAR_index: double (nullable = true)\n",
      " |-- FLAG_OWN_REALTY_index: double (nullable = true)\n",
      " |-- NAME_TYPE_SUITE_index: double (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_index: double (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_index: double (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_index: double (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE_index: double (nullable = true)\n",
      " |-- OCCUPATION_TYPE_index: double (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_index: double (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_index: double (nullable = true)\n",
      " |-- FONDKAPREMONT_MODE_index: double (nullable = true)\n",
      " |-- HOUSETYPE_MODE_index: double (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_index: double (nullable = true)\n",
      " |-- EMERGENCYSTATE_MODE_index: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV\n",
    "df_filled = spark.read.csv(\"E:/Class/sem3/big data framework/Project/df_filled.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame schema and a few rows to confirm successful loading\n",
    "df_filled.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate features and target\n",
    "target_col = \"TARGET\"\n",
    "X = df_filled.drop(target_col)\n",
    "y = df_filled.select(\"SK_ID_CURR\", target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = [field.name for field in X.schema.fields if field.dataType.simpleString() == 'string']\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = [field.name for field in X.schema.fields if field.dataType.simpleString() in ['int', 'double']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Drop existing index columns if they exist to avoid conflicts\n",
    "existing_columns = set(X.columns)\n",
    "index_columns = [col + '_index' for col in categorical_columns]\n",
    "columns_to_drop = [col for col in index_columns if col in existing_columns]\n",
    "\n",
    "if columns_to_drop:\n",
    "    X = X.drop(*columns_to_drop)\n",
    "\n",
    "# Define indexers for categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_index') for col in categorical_columns]\n",
    "\n",
    "# Define the assembler with indexed and numerical columns\n",
    "indexed_columns = [col + '_index' for col in categorical_columns]\n",
    "assembler = VectorAssembler(inputCols=indexed_columns + numerical_columns, outputCol='features')\n",
    "\n",
    "# Create and fit the pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "\n",
    "# Apply the pipeline\n",
    "X_transformed = pipeline.fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# `SK_ID_CURR` is a unique identifier in your original DataFrame\n",
    "X_transformed = X_transformed.withColumn(\"SK_ID_CURR\", X[\"SK_ID_CURR\"])\n",
    "\n",
    "# Now join the target column back into the transformed DataFrame\n",
    "final_df = X_transformed.join(y.withColumnRenamed(target_col, \"TARGET\"), on=\"SK_ID_CURR\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SK_ID_CURR: double, NAME_CONTRACT_TYPE: string, CODE_GENDER: string, FLAG_OWN_CAR: string, FLAG_OWN_REALTY: string, CNT_CHILDREN: double, AMT_INCOME_TOTAL: double, AMT_CREDIT: double, AMT_ANNUITY: double, AMT_GOODS_PRICE: double, NAME_TYPE_SUITE: string, NAME_INCOME_TYPE: string, NAME_EDUCATION_TYPE: string, NAME_FAMILY_STATUS: string, NAME_HOUSING_TYPE: string, REGION_POPULATION_RELATIVE: double, DAYS_BIRTH: double, DAYS_EMPLOYED: double, DAYS_REGISTRATION: double, DAYS_ID_PUBLISH: double, OWN_CAR_AGE: double, FLAG_MOBIL: double, FLAG_EMP_PHONE: double, FLAG_WORK_PHONE: double, FLAG_CONT_MOBILE: double, FLAG_PHONE: double, FLAG_EMAIL: double, OCCUPATION_TYPE: string, CNT_FAM_MEMBERS: double, REGION_RATING_CLIENT: double, REGION_RATING_CLIENT_W_CITY: double, WEEKDAY_APPR_PROCESS_START: string, HOUR_APPR_PROCESS_START: double, REG_REGION_NOT_LIVE_REGION: double, REG_REGION_NOT_WORK_REGION: double, LIVE_REGION_NOT_WORK_REGION: double, REG_CITY_NOT_LIVE_CITY: double, REG_CITY_NOT_WORK_CITY: double, LIVE_CITY_NOT_WORK_CITY: double, ORGANIZATION_TYPE: string, EXT_SOURCE_1: double, EXT_SOURCE_2: double, EXT_SOURCE_3: double, APARTMENTS_AVG: double, BASEMENTAREA_AVG: double, YEARS_BEGINEXPLUATATION_AVG: double, YEARS_BUILD_AVG: double, COMMONAREA_AVG: double, ELEVATORS_AVG: double, ENTRANCES_AVG: double, FLOORSMAX_AVG: double, FLOORSMIN_AVG: double, LANDAREA_AVG: double, LIVINGAPARTMENTS_AVG: double, LIVINGAREA_AVG: double, NONLIVINGAPARTMENTS_AVG: double, NONLIVINGAREA_AVG: double, APARTMENTS_MODE: double, BASEMENTAREA_MODE: double, YEARS_BEGINEXPLUATATION_MODE: double, YEARS_BUILD_MODE: double, COMMONAREA_MODE: double, ELEVATORS_MODE: double, ENTRANCES_MODE: double, FLOORSMAX_MODE: double, FLOORSMIN_MODE: double, LANDAREA_MODE: double, LIVINGAPARTMENTS_MODE: double, LIVINGAREA_MODE: double, NONLIVINGAPARTMENTS_MODE: double, NONLIVINGAREA_MODE: double, APARTMENTS_MEDI: double, BASEMENTAREA_MEDI: double, YEARS_BEGINEXPLUATATION_MEDI: double, YEARS_BUILD_MEDI: double, COMMONAREA_MEDI: double, ELEVATORS_MEDI: double, ENTRANCES_MEDI: double, FLOORSMAX_MEDI: double, FLOORSMIN_MEDI: double, LANDAREA_MEDI: double, LIVINGAPARTMENTS_MEDI: double, LIVINGAREA_MEDI: double, NONLIVINGAPARTMENTS_MEDI: double, NONLIVINGAREA_MEDI: double, FONDKAPREMONT_MODE: string, HOUSETYPE_MODE: string, TOTALAREA_MODE: double, WALLSMATERIAL_MODE: string, EMERGENCYSTATE_MODE: string, OBS_30_CNT_SOCIAL_CIRCLE: double, DEF_30_CNT_SOCIAL_CIRCLE: double, OBS_60_CNT_SOCIAL_CIRCLE: double, DEF_60_CNT_SOCIAL_CIRCLE: double, DAYS_LAST_PHONE_CHANGE: double, FLAG_DOCUMENT_2: double, FLAG_DOCUMENT_3: double, FLAG_DOCUMENT_4: double, FLAG_DOCUMENT_5: double, FLAG_DOCUMENT_6: double, FLAG_DOCUMENT_7: double, FLAG_DOCUMENT_8: double, FLAG_DOCUMENT_9: double, FLAG_DOCUMENT_10: double, FLAG_DOCUMENT_11: double, FLAG_DOCUMENT_12: double, FLAG_DOCUMENT_13: double, FLAG_DOCUMENT_14: double, FLAG_DOCUMENT_15: double, FLAG_DOCUMENT_16: double, FLAG_DOCUMENT_17: double, FLAG_DOCUMENT_18: double, FLAG_DOCUMENT_19: double, FLAG_DOCUMENT_20: double, FLAG_DOCUMENT_21: double, AMT_REQ_CREDIT_BUREAU_HOUR: double, AMT_REQ_CREDIT_BUREAU_DAY: double, AMT_REQ_CREDIT_BUREAU_WEEK: double, AMT_REQ_CREDIT_BUREAU_MON: double, AMT_REQ_CREDIT_BUREAU_QRT: double, AMT_REQ_CREDIT_BUREAU_YEAR: double, NAME_CONTRACT_TYPE_index: double, CODE_GENDER_index: double, FLAG_OWN_CAR_index: double, FLAG_OWN_REALTY_index: double, NAME_TYPE_SUITE_index: double, NAME_INCOME_TYPE_index: double, NAME_EDUCATION_TYPE_index: double, NAME_FAMILY_STATUS_index: double, NAME_HOUSING_TYPE_index: double, OCCUPATION_TYPE_index: double, WEEKDAY_APPR_PROCESS_START_index: double, ORGANIZATION_TYPE_index: double, FONDKAPREMONT_MODE_index: double, HOUSETYPE_MODE_index: double, WALLSMATERIAL_MODE_index: double, EMERGENCYSTATE_MODE_index: double, features: vector, TARGET: int]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data into training and testing sets\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             feature|          importance|\n",
      "+--------------------+--------------------+\n",
      "|   BASEMENTAREA_MODE|  0.1713663968709113|\n",
      "|   NONLIVINGAREA_AVG|  0.1276040331901643|\n",
      "|     APARTMENTS_MODE| 0.11905245612042939|\n",
      "|LIVE_REGION_NOT_W...| 0.03503646182159677|\n",
      "|   CODE_GENDER_index| 0.03479505163720913|\n",
      "|REGION_POPULATION...| 0.03395462150746875|\n",
      "|    FLAG_DOCUMENT_17| 0.03235152254686104|\n",
      "|       ELEVATORS_AVG|0.030672904157936512|\n",
      "|     AMT_GOODS_PRICE|0.029840263483708922|\n",
      "|        EXT_SOURCE_2|0.026322980955377488|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Top 10 Features:\n",
      "BASEMENTAREA_MODE\n",
      "NONLIVINGAREA_AVG\n",
      "APARTMENTS_MODE\n",
      "LIVE_REGION_NOT_WORK_REGION\n",
      "CODE_GENDER_index\n",
      "REGION_POPULATION_RELATIVE\n",
      "FLAG_DOCUMENT_17\n",
      "ELEVATORS_AVG\n",
      "AMT_GOODS_PRICE\n",
      "EXT_SOURCE_2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#  Create a Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=target_col, featuresCol='features', numTrees=100, maxBins=64)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = rf_model.featureImportances.toArray()  # Convert to a numpy array and then to a list\n",
    "\n",
    "#  Extract feature names from the assembler (assuming you created a feature vector with an assembler)\n",
    "# If you used an assembler, you may have saved the feature names in a list; use that.\n",
    "feature_names = numerical_columns + [f\"{col}_index\" for col in categorical_columns]\n",
    "\n",
    "# Create a DataFrame of features and their importances\n",
    "# Convert importances to a list of floats for compatibility with PySpark\n",
    "importance_list = [float(importance) for importance in importances]\n",
    "\n",
    "# Create DataFrame using the feature names and importances\n",
    "feature_importance_df = spark.createDataFrame(zip(feature_names, importance_list), schema=['feature', 'importance'])\n",
    "\n",
    "#  Sort by importance and select the top 10 features\n",
    "top_features = feature_importance_df.orderBy(F.col(\"importance\").desc()).limit(10)\n",
    "\n",
    "# Show the top features with their importance values\n",
    "top_features.show()\n",
    "\n",
    "# Print out the column names of the top features\n",
    "top_feature_names = [row.feature for row in top_features.collect()]\n",
    "print(\"Top 10 Features:\")\n",
    "for name in top_feature_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model without handling unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Top Features: 0.5\n",
      "+------+----------+--------------------+\n",
      "|TARGET|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       0.0|[0.89622115646501...|\n",
      "|     0|       0.0|[0.94429391902422...|\n",
      "|     0|       0.0|[0.93334262100710...|\n",
      "|     0|       0.0|[0.96083280393971...|\n",
      "|     0|       0.0|[0.9344689514534,...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# Create a new VectorAssembler with the top features\n",
    "top_feature_assembler = VectorAssembler(inputCols=top_feature_names, outputCol='top_features')\n",
    "\n",
    "#  Create a Logistic Regression model\n",
    "lr_top_features = LogisticRegression(labelCol=target_col, featuresCol='top_features')\n",
    "\n",
    "#  Create a pipeline\n",
    "pipeline_top_features = Pipeline(stages=[top_feature_assembler, lr_top_features])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model_top_features = pipeline_top_features.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_top_features = model_top_features.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions_top_features)\n",
    "print(f\"Accuracy with Top Features: {accuracy}\")\n",
    "\n",
    "# Optional: Show some predictions along with actual values\n",
    "predictions_top_features.select(target_col, \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8804876564778787\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming `predictions_top_features` contains the predictions from the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = evaluator.evaluate(predictions_top_features)\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of loan default: 19887\n",
      "Count of paying back the loan: 226517\n",
      "Balanced train count of loan default: 19999\n",
      "Balanced train count of paying back the loan: 19868\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Count the number of instances in each class\n",
    "count_positive = train_df.filter(col(\"TARGET\") == 1).count()\n",
    "count_negative = train_df.filter(col(\"TARGET\") == 0).count()\n",
    "\n",
    "# Print counts for debugging\n",
    "print(f\"Count of loan default: {count_positive}\")\n",
    "print(f\"Count of paying back the loan: {count_negative}\")\n",
    "\n",
    "# Create DataFrames for each class\n",
    "df_positive = train_df.filter(col(\"TARGET\") == 1)\n",
    "df_negative = train_df.filter(col(\"TARGET\") == 0)\n",
    "\n",
    "# Undersample the majority class (negative instances)\n",
    "df_negative_sampled = df_negative.sample(fraction=count_positive / count_negative, seed=42)\n",
    "\n",
    "# Combine the sampled negative instances with the positive instances\n",
    "balanced_train_df = df_positive.union(df_negative_sampled)\n",
    "\n",
    "# Verify the counts after undersampling\n",
    "print(f\"Balanced train count of loan default: {balanced_train_df.filter(col('TARGET') == 1).count()}\")\n",
    "print(f\"Balanced train count of paying back the loan: {balanced_train_df.filter(col('TARGET') == 0).count()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression after handling the imbalnced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Undersampled Data and Top Features: 0.6190141723617585\n",
      "+------+----------+--------------------+\n",
      "|TARGET|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.43540010487094...|\n",
      "|     0|       0.0|[0.59697974186368...|\n",
      "|     0|       0.0|[0.54713208450799...|\n",
      "|     0|       0.0|[0.67120078431392...|\n",
      "|     0|       0.0|[0.55213074659830...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Create a Logistic Regression model using the top features\n",
    "lr_top_features = LogisticRegression(labelCol=target_col, featuresCol='top_features')\n",
    "\n",
    "#  Create a pipeline with the top feature assembler and the logistic regression model\n",
    "pipeline_top_features = Pipeline(stages=[top_feature_assembler, lr_top_features])\n",
    "\n",
    "#  Fit the model on the balanced training data\n",
    "model_top_features = pipeline_top_features.fit(balanced_train_df)\n",
    "\n",
    "#  Make predictions on the test data\n",
    "predictions_top_features = model_top_features.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions_top_features)\n",
    "print(f\"Accuracy with Undersampled Data and Top Features: {accuracy}\")\n",
    "\n",
    "# Show some predictions along with actual values\n",
    "predictions_top_features.select(target_col, \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7300812616046058\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#  Calculate the F1 score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions_top_features)\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved at: E:\\Class\\sem3\\big data framework\\Project\\lr_top_features_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the model to a specified path\n",
    "model_path = r\"E:\\Class\\sem3\\big data framework\\Project\\lr_top_features_model\"  \n",
    "lr_top_features.save(model_path)\n",
    "\n",
    "print(f\"Logistic Regression model saved at: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Undersampled Data and Top Features (GBT): 0.6303089517624157\n",
      "+------+----------+--------------------+\n",
      "|TARGET|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.43127460403293...|\n",
      "|     0|       0.0|[0.59570018322394...|\n",
      "|     0|       0.0|[0.55864029942773...|\n",
      "|     0|       0.0|[0.56720905562465...|\n",
      "|     0|       1.0|[0.48154104339152...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a GradientBoostedTreesClassifier model using the top features\n",
    "gbt_top_features = GBTClassifier(labelCol=target_col, featuresCol='top_features')\n",
    "\n",
    "# Create a pipeline with the top feature assembler and the GradientBoostedTrees model\n",
    "pipeline_top_features_gbt = Pipeline(stages=[top_feature_assembler, gbt_top_features])\n",
    "\n",
    "# Fit the model on the balanced training data\n",
    "model_top_features_gbt = pipeline_top_features_gbt.fit(balanced_train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_top_features_gbt = model_top_features_gbt.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions_top_features_gbt)\n",
    "print(f\"Accuracy with Undersampled Data and Top Features (GBT): {accuracy}\")\n",
    "\n",
    "# Show some predictions along with actual values\n",
    "predictions_top_features_gbt.select(target_col, \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score GBT: 0.7096900767524533\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#  Calculate the F1 score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions_top_features_gbt)\n",
    "print(f\"F1 Score GBT: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Undersampled Data and Top Features (RandomForest): 0.6250871702125639\n",
      "+------+----------+--------------------+\n",
      "|TARGET|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|     1|       1.0|[0.47267737724594...|\n",
      "|     0|       0.0|[0.61325089572656...|\n",
      "|     0|       0.0|[0.61071776933007...|\n",
      "|     0|       0.0|[0.57414863286663...|\n",
      "|     0|       0.0|[0.55399393756250...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a RandomForestClassifier model using the top features\n",
    "rf_top_features = RandomForestClassifier(labelCol=target_col, featuresCol='top_features')\n",
    "\n",
    "# Create a pipeline with the top feature assembler and the RandomForest model\n",
    "pipeline_top_features_rf = Pipeline(stages=[top_feature_assembler, rf_top_features])\n",
    "\n",
    "# Fit the model on the balanced training data\n",
    "model_top_features_rf = pipeline_top_features_rf.fit(balanced_train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_top_features_rf = model_top_features_rf.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions_top_features_rf)\n",
    "print(f\"Accuracy with Undersampled Data and Top Features (RandomForest): {accuracy}\")\n",
    "\n",
    "# Show some predictions along with actual values\n",
    "predictions_top_features_rf.select(target_col, \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score RandomForest: 0.7201549208474419\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#  Calculate the F1 score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions_top_features_rf)\n",
    "print(f\"F1 Score RandomForest: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the pipeline model\n",
    "pipeline_top_features_rf.write().overwrite().save(r\"E:\\Class\\sem3\\big data framework\\Project\\pipeline_model\")\n",
    "\n",
    "# Save the RandomForest model\n",
    "model_top_features_rf.stages[-1].write().overwrite().save(r\"E:\\Class\\sem3\\big data framework\\Project\\rf_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
